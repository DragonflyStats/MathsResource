http://users.cis.fiu.edu/~downeyt/cop3402/hamming.html

%=========================================================================================%

Hamming code is a set of error-correction code s that can be used to detect and correct bit errors that can occur when computer data is moved or stored. Hamming code is named for R. W. Hamming of Bell Labs.

Like other error-correction code, Hamming code makes use of the concept of parity and parity bit s, which are bits that are added to data so that the validity of the data can be checked when it is read or after it has been received in a data transmission. Using more than one parity bit, an error-correction code can not only identify a single bit error in the data unit, but also its location in the data unit.

In data transmission, the ability of a receiving station to correct errors in the received data is called forward error correction (FEC) and can increase throughput on a data link when there is a lot of noise present. To enable this, a transmitting station must add extra data (called error correction bits ) to the transmission. However, the correction may not always represent a cost saving over that of simply resending the information. Hamming codes make FEC less expensive to implement through the use of a block parity mechanism.

Computing parity involves counting the number of ones in a unit of data, and adding either a zero or a one (called a parity bit ) to make the count odd (for odd parity) or even (for even parity). For example, 1001 is a 4-bit data unit containing two one bits; since that is an even number, a zero would be added to maintain even parity, or, if odd parity was being maintained, another one would be added. To calculate even parity, the XOR operator is used; to calculate odd parity, the XNOR operator is used. Single bit errors are detected when the parity count indicates that the number of ones is incorrect, indicating that a data bit has been flipped by noise in the line. Hamming codes detect two bit errors by using more than one parity bit, each of which is computed on different combinations of bits in the data. The number of parity bits required depends on the number of bits in the data transmission, and is calculated by the Hamming rule:

p
d + p + 1 < = 2 (1)

Where d is the number of data bits and p is the number of parity bits. The total of the two is called the Hamming code word, which is generated by multiplying the data bits by a generator matrix .

%===================================================================================%

Code & Codeword

Let A be a finite set, called alphabet; it should have at least two different elements. Let n be a natural number. A code of length n over alphabet A is any set C of n-long sequences of elements from A; the sequences from C are called codewords of C.
If alphabet A = {0,1} then codes over A are called binary.

For example, let C be a code over alphabet A := GF(5) := {0,1,2,3,4}. Let C := {000,111,222,333,444}. It has codewords 000, 111, 222, 333 and 444.

Now we should discuss some properties of a code. Firstly, we can have the notion of distance between two codewords.

Hamming Distance

Let C be a code, and x and y (bold to signify that each codeword is like a vector) are codewords of C. The Hamming distance of x and y denoted
d(x,y)
is the number places in which x and y differ.
E.g. d(000,111) = 3.

Hamming distance enjoys the following three fundamental metric properties:

d(x,y) = 0 <==> 'x' = 'y'
d(x,y) = d(y,x)
d(x,y) â‰¤ d(x,z) + d(z,y); triangle inequality
Minimum distance

The minimum distance of a code C denoted d(C) is the minimum distance possible between two different codewords of C
E.g. Let C = {000,111,110,001}, then d(C) = d(000,001) = 1, as the distance between any other codewords are greater than or equal to 1.

The significance of minimum distance[edit]
The minimum distance of a code C is closely related to its ability to correct errors. Let's illustrate why, using a hypothetical code C. Let say this code has minimum distance 5, i.e. d(C) = 5. If a codeword, x is sent and only up to 2 errors were introduced in transmission, then it can be corrected.

Suppose x is sent but x + e is received, where e corresponds to some vector with up to 2 non-zero components. We see that x + e is closer to x than any other codeword! This is due to the fact that d(C) = 5.

E.g. let C = {00000,11111,22222} and 00000 is sent but 00012 is received. It is easy to see that 00000 is the closet codeword to 00012. So we decode 00012 as 00000, we have in effect corrected 2 errors. But if 3 or more errors are made and we decode using the closest codeword, then we might be in trouble. E.g. if 11111 is sent but 11222 is received. We decode 11222 as 22222, but this is wrong!

No error correcting code is perfect (although we call some perfect codes). No code can correct every possible error vector. But it is also reasonable to assume that only a small number of errors are made each transmission and so we only need codes that can correct a small number of errors.
